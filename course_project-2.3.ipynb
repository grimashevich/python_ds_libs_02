{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Новые признаки:\n",
    "1. Была ли раньше просрочка или нет (на основании Months since last delinquent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import classification_report, f1_score, precision_score, recall_score\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classification_report(y_train_true, y_train_pred, y_test_true, y_test_pred):\n",
    "    print('TRAIN\\n\\n' + classification_report(y_train_true, y_train_pred))\n",
    "    print('TEST\\n\\n' + classification_report(y_test_true, y_test_pred))\n",
    "    print('CONFUSION MATRIX\\n')\n",
    "    print(pd.crosstab(y_test_true, y_test_pred))\n",
    "\n",
    "\n",
    "def evaluate_preds(model, X_train, X_test, y_train, y_test):\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    get_classification_report(y_train, y_train_pred, y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li><b>Home Ownership</b> - домовладение</li>\n",
    "<li><b>Annual Income</b> - годовой доход</li>\n",
    "<li><b>Years in current job</b> - количество лет на текущем месте работы</li>\n",
    "<li><b>Tax Liens</b> - налоговые обременения</li>\n",
    "<li><b>Number of Open Accounts</b> - количество открытых счетов</li>\n",
    "<li><b>Years of Credit History</b> - количество лет кредитной истории</li>\n",
    "<li><b>Maximum Open Credit</b> - наибольший открытый кредит</li>\n",
    "<li><b>Number of Credit Problems</b> - количество проблем с кредитом</li>\n",
    "<li><b>Months since last delinquent</b> - количество месяцев с последней просрочки платежа</li>\n",
    "<li><b>Bankruptcies</b> - банкротства</li>\n",
    "<li><b>Purpose</b> - цель кредита</li>\n",
    "<li><b>Term</b> - срок кредита</li>\n",
    "<li><b>Current Loan Amount</b> - текущая сумма кредита</li>\n",
    "<li><b>Current Credit Balance</b> - текущий кредитный баланс</li>\n",
    "<li><b>Monthly Debt</b> - ежемесячный долг</li>\n",
    "<li><b>Credit Score</b> - баллы кредитного рейтинга</li>\n",
    "<li><b>Credit Default</b> - факт невыполнения кредитных обязательств (0 - погашен вовремя, 1 - просрочка)</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Корреляция признаков "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_with_target = df.corr().iloc[:-1, -1].sort_values(ascending=False)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(x=corr_with_target.values, y=corr_with_target.index)\n",
    "plt.title('Корреляция признаков с целевой переменной')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Credit Score'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['Credit Score'] > 600) & (df['Credit Score'] < 700) & (df['Credit Default'] == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(df[(df['Credit Score'] > 751)]['Credit Default'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Credit Score'] < 1000]['Credit Score'].hist(bins = 10, figsize=(40,30))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 8))\n",
    "\n",
    "sns.countplot(x='Credit Score', hue='Credit Default', data=df[(df['Credit Score'] >= 0) & (df['Credit Score'] < 800)])\n",
    "plt.title('PAY_1 grouped by target variable')\n",
    "plt.legend(title='Target', loc='upper right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorial_features = ['Home Ownership', 'Years in current job', 'Purpose', 'Term']\n",
    "numerical_features = ['Annual Income', 'Tax Liens','Number of Open Accounts', 'Years of Credit History',\n",
    "                      'Maximum Open Credit', 'Number of Credit Problems', 'Months since last delinquent', 'Bankruptcies',\n",
    "                      'Current Loan Amount', 'Current Credit Balance', 'Monthly Debt', 'Credit Score']\n",
    "all_features = categorial_features + numerical_features\n",
    "target_feature = ['Credit Default']\n",
    "\n",
    "# No NaN numerical features\n",
    "nnn_features = ['Tax Liens','Number of Open Accounts', 'Years of Credit History',\n",
    "                      'Maximum Open Credit', 'Number of Credit Problems',\n",
    "                      'Current Loan Amount', 'Current Credit Balance', 'Monthly Debt']\n",
    "\n",
    "wnn_features = ['Annual Income', 'Months since last delinquent', 'Bankruptcies', 'Credit Score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Классы дисбалансированы, поэтому основная идея - удалить строки с nan значениями, которые относятся к нулевому классу, а потом применить SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Удалим признаки с пропусками, относящиеся к нулевой группе, кроме Months since last delinquent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_na = (df['Annual Income'].isna() | df['Bankruptcies'].isna() | df['Credit Score'].isna()) & (df['Credit Default'] == 0)\n",
    "df = df.drop(df[ft_na].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Считаю важным признаком Months since last delinquent, поэтому для хорошей разделимости NaN значения заполним цифрой 999, а так же создадим новый логический признак по этому параметру"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modyfy_MSL(df1):\n",
    "    df1['Never delinquent'] = 0\n",
    "    df1.loc[df1['Months since last delinquent'].isna(), 'Never delinquent'] = 1\n",
    "    df1.loc[df1['Months since last delinquent'].isna(), 'Months since last delinquent'] = 999\n",
    "    return df1\n",
    "\n",
    "df = modyfy_MSL(df)\n",
    "numerical_features += ['Never delinquent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info() # ВРЕМЕННАЯ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Заполнение пропусков при помощи регрессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_nan_rfr(X, y):\n",
    "    \"\"\"Ищет лучшие параметры\"\"\"\n",
    "    rfr = RandomForestRegressor()\n",
    "    parametrs = { 'n_estimators': [1000],\n",
    "                  'max_depth': [9],\n",
    "                  'min_samples_leaf': range(3,6),\n",
    "                  'min_samples_split': range(3,6) }\n",
    "\n",
    "    grid = GridSearchCV(rfr, parametrs, cv=5, n_jobs=-1)\n",
    "    grid.fit(X, y)\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_features = nnn_features + ['Credit Default', 'Home Ownership', 'Purpose', 'Term'] + target_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# a = fill_nan_rfr(df[~df['Annual Income'].isna()][X_features], df[~df['Annual Income'].isna()]['Annual Income'])\n",
    "best_params = {'max_depth': 9,\n",
    "               'min_samples_leaf': 3,\n",
    "               'min_samples_split': 3,\n",
    "               'n_estimators': 1000,\n",
    "               'n_jobs': -1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_outlier(df1):\n",
    "    df1['CS_out'] = 0\n",
    "    df1.loc[df1['Credit Score'] >= 800, 'CS_out'] = 1\n",
    "    df1.loc[df1['Credit Score'] >= 800, 'Credit Score'] = np.nan\n",
    "    return df1\n",
    "\n",
    "\n",
    "def simple_label_encoder(df1, cats=categorial_features):\n",
    "    le = LabelEncoder()\n",
    "    for i in cats:\n",
    "        le.fit(df1[i].astype(str))\n",
    "        df1[i] = le.transform(df1[i].astype(str))\n",
    "    return df1\n",
    "\n",
    "\n",
    "def simple_fill_na(df1):\n",
    "    for i in df1.columns:\n",
    "        df1[i].fillna(df1[i].median(), inplace=True)\n",
    "    return df1\n",
    "\n",
    "def pred_nan(X_df, target, cats=categorial_features, params=best_params):\n",
    "    X_df1 = X_df.copy()\n",
    "    X_df1 = simple_label_encoder(X_df1)\n",
    "    y = X_df1[target]\n",
    "    X = X_df1.drop(target, axis=1)\n",
    "    X = simple_fill_na(X)\n",
    "    X_train = X[~y.isna()]\n",
    "    y_train = y[~y.isna()]\n",
    "    X_pred = X[y.isna()]\n",
    "    rfr = RandomForestRegressor(**params)\n",
    "    rfr.fit(X_train, y_train)\n",
    "    pred = rfr.predict(X_pred)\n",
    "    X_df.loc[X_df[target].isna(), target] = pred\n",
    "    return X_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def fill_all_na(df1):\n",
    "    df1 = fix_outlier(df1)\n",
    "    df1 = pred_nan(df1, 'Annual Income')\n",
    "    df1 = pred_nan(df1, 'Bankruptcies')\n",
    "    df1 = pred_nan(df1, 'Credit Score')\n",
    "    df1.loc[df1['Years in current job'].isna(),'Years in current job'] = df1['Years in current job'].mode()[0]\n",
    "    \n",
    "    return df1\n",
    "df = fill_all_na(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs_groups = [[0, 670]] + [[i, i+10] for i in range(670, 740, 10)] + [[740, 9999]]\n",
    "\n",
    "cs_groups_test = []\n",
    "for i, j in cs_groups:\n",
    "    df_filter = (df['Credit Score'] >= i) & (df['Credit Score'] < j)\n",
    "    cnt = dict(df[df_filter]['Credit Default'].value_counts())\n",
    "    cd_sum = cnt.setdefault(0, 0) + cnt.setdefault(1, 0)\n",
    "    cd_perc = cnt[1] / cd_sum\n",
    "    cs_groups_test.append([i, j, cd_perc])\n",
    "    df.loc[df_filter, 'target_per_CS'] = cd_perc\n",
    "    \n",
    "def mean_test_cs_groups(df1, cs_groups=cs_groups_test):\n",
    "    df1['target_per_CS'] = 0\n",
    "    for i, j, c in cs_groups:\n",
    "        df_filter = (df1['Credit Score'] >= i) & (df1['Credit Score'] < j)\n",
    "        df1.loc[df_filter, 'target_per_CS'] = c\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Кодирование категориальных признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = simple_label_encoder(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(bins = 30, figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df.corr());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classification_report(y_train_true, y_train_pred, y_test_true, y_test_pred):\n",
    "    print('TRAIN\\n\\n' + classification_report(y_train_true, y_train_pred))\n",
    "    print('TEST\\n\\n' + classification_report(y_test_true, y_test_pred))\n",
    "    print('CONFUSION MATRIX\\n')\n",
    "    print(pd.crosstab(y_test_true, y_test_pred))\n",
    "\n",
    "\n",
    "def evaluate_preds(model, X_train, X_test, y_train, y_test):\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    get_classification_report(y_train, y_train_pred, y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Разбиение на трейн и тест"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns='Credit Default')\n",
    "y = df['Credit Default']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True,\n",
    "                                                    random_state=1, stratify=df['Credit Default'])\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE()\n",
    "X_smote, y_smote = smote.fit_sample(X, y)\n",
    "X_train_smote, y_train_smote = smote.fit_sample(X_train, y_train)\n",
    "# df = pd.concat([X_smote, y_smote], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(random_state=1,\n",
    "                             max_depth=3,\n",
    "                             n_estimators=512,\n",
    "                             min_samples_split = 2,\n",
    "                             min_samples_leaf = 1,\n",
    "                             n_jobs=-1)\n",
    "\n",
    "rfc = RandomForestClassifier(random_state=1, max_depth=5, n_estimators=99, n_jobs=-1)\n",
    "\n",
    "rfc.fit(X_train_smote, y_train_smote)\n",
    "y_test_pred = rfc.predict(X_test)\n",
    "print(f1_score(y_test, y_test_pred))\n",
    "evaluate_preds(rfc, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = modyfy_MSL(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_test = fill_all_na(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = simple_label_encoder(df_test)\n",
    "df_test = mean_test_cs_groups(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc.fit(X_smote, y_smote)\n",
    "df_test_pred = pd.DataFrame()\n",
    "df_test_pred['Id'] = list(df_test.index)\n",
    "df_test_pred['Credit Default'] = rfc.predict(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_pred.to_csv('final_pred_v2-3.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Credit Default'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=105, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=105\n",
      "TRAIN\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.64      0.74      3481\n",
      "           1       0.52      0.79      0.62      1690\n",
      "\n",
      "    accuracy                           0.69      5171\n",
      "   macro avg       0.69      0.71      0.68      5171\n",
      "weighted avg       0.75      0.69      0.70      5171\n",
      "\n",
      "TEST\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.64      0.73       870\n",
      "           1       0.51      0.78      0.62       423\n",
      "\n",
      "    accuracy                           0.68      1293\n",
      "   macro avg       0.68      0.71      0.67      1293\n",
      "weighted avg       0.74      0.68      0.69      1293\n",
      "\n",
      "CONFUSION MATRIX\n",
      "\n",
      "col_0             0    1\n",
      "Credit Default          \n",
      "0               555  315\n",
      "1                93  330\n",
      "Wall time: 59 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import lightgbm as lgbm\n",
    "model_lgbm = lgbm.LGBMClassifier(random_state=21, \n",
    "                                 is_unbalance=True,\n",
    "                                 n_estimators=25,\n",
    "                                 min_data_in_leaf=105,\n",
    "                                 num_leaves=5,\n",
    "                                 learning_rate=0.15,\n",
    "                                )\n",
    "model_lgbm.fit(X_train, y_train)\n",
    "\n",
    "evaluate_preds(model_lgbm, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=105, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=105\n"
     ]
    }
   ],
   "source": [
    "model_lgbm.fit(X, y)\n",
    "df_test_pred = pd.DataFrame()\n",
    "df_test_pred['Id'] = list(df_test.index)\n",
    "df_test_pred['Credit Default'] = model_lgbm.predict(df_test)\n",
    "df_test_pred.to_csv('final_pred_v2-3_LGBM.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'num_leaves': [31, 127],\n",
    "#     'reg_alpha': [0.1, 0.5],\n",
    "    'min_data_in_leaf': [30, 50, 100, 300, 400],\n",
    "#     'lambda_l1': [0, 1,1.5],\n",
    "#     'lambda_l2': [0, 1]\n",
    "    }\n",
    "\n",
    "lgb_estimator = lgbm.LGBMClassifier(boosting_type='gbdt',\n",
    "                                   objective='binary',\n",
    "                                   learning_rate=0.01,\n",
    "                                   metric='f1')\n",
    "\n",
    "gsearch = GridSearchCV(estimator=lgb_estimator, param_grid=param_grid, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_model = gsearch.fit(X=X, y=y)\n",
    "\n",
    "print(lgb_model.best_params_, lgb_model.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "model_xgb = xgb.XGBClassifier(random_state=21, \n",
    "                              max_depth=1, \n",
    "                              )\n",
    "model_xgb.fit(X_train_smote, y_train_smote)\n",
    "y_test_pred = model_xgb.predict(X_test)\n",
    "print(f1_score(y_test, y_test_pred))\n",
    "evaluate_preds(model_xgb, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = modyfy_MSL(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_test = fill_all_na(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = simple_label_encoder(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_pred = pd.DataFrame()\n",
    "df_test_pred['Id'] = list(df_test.index)\n",
    "df_test_pred['Credit Default'] = rfc.predict(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_pred.to_csv('final_pred_v2-1XGB.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 - добавил smote() # 0.43621<br><br><br>\n",
    "2.1 - изменил параметры: rfc = RandomForestClassifier(random_state=1, max_depth=3, n_estimators=9, n_jobs=-1)<br>\n",
    "\\# 0.49063<br>\n",
    "TRAIN\n",
    "\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.75      0.96      0.84      3492\n",
    "           1       0.94      0.68      0.79      3469\n",
    "\n",
    "    accuracy                           0.82      6961\n",
    "   macro avg       0.84      0.82      0.81      6961\n",
    "weighted avg       0.84      0.82      0.81      6961\n",
    "\n",
    "TEST\n",
    "\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.75      0.95      0.84       859\n",
    "           1       0.93      0.69      0.79       882\n",
    "\n",
    "    accuracy                           0.82      1741\n",
    "   macro avg       0.84      0.82      0.81      1741\n",
    "weighted avg       0.84      0.82      0.81      1741\n",
    "\n",
    "CONFUSION MATRIX\n",
    "\n",
    "col_0             0    1\n",
    "Credit Default          \n",
    "0               815   44\n",
    "1               276  606"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import catboost as catb\n",
    "frozen_params = {\n",
    "     'silent':True,\n",
    "     'random_state':21,\n",
    "     'cat_features':categorial_features,\n",
    "     'eval_metric':'F1',\n",
    "     'early_stopping_rounds':20\n",
    "}\n",
    "\n",
    "model_catb = catb.CatBoostClassifier(**frozen_params, iterations=200, max_depth=5)\n",
    "model_catb.fit(X_train, y_train, plot=True, eval_set=(X_test, y_test))\n",
    "\n",
    "evaluate_preds(model_catb, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[df_test['Credit Score'] > 751]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = modyfy_MSL(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_test = fill_all_na(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = simple_label_encoder(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_pred = pd.DataFrame()\n",
    "df_test_pred['Id'] = list(df_test.index)\n",
    "df_test_pred['Credit Default'] = model_catb.predict(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_pred.to_csv('final_pred_v2-2_catb.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
